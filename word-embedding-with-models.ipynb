{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b58b195-de0c-48ee-8a08-645fc0026384",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73bc86-0e4d-44bd-81dc-ca7d85bcb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Attention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('english_only.csv') \n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATE_SIZE = 0.1765\n",
    "RANDOM_STATE_INT = 14988828\n",
    "batch_size = 32\n",
    "epochs = 10  \n",
    "\n",
    "max_len = 128\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(df['excerpt_value_cleaned'])\n",
    "sequences = tokenizer.texts_to_sequences(df['excerpt_value_cleaned'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['plutchik_emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE_INT)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATE_SIZE, random_state=RANDOM_STATE_INT)\n",
    "\n",
    "def load_glove_embeddings(glove_file_path, embedding_dim, tokenizer_word_index):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer_word_index) + 1, embedding_dim))\n",
    "    for word, i in tokenizer_word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 300\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index)\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
    "bi_lstm = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
    "\n",
    "attention = Attention()([bi_lstm, bi_lstm])\n",
    "context_vector = GlobalAveragePooling1D()(attention)\n",
    "\n",
    "dropout_layer = Dropout(dropout_rate)(context_vector)\n",
    "dense_layer_1 = Dense(64, activation='relu')(dropout_layer)\n",
    "dropout_layer_2 = Dropout(dropout_rate)(dense_layer_1)\n",
    "output_layer = Dense(8, activation='softmax')(dropout_layer_2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_w = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_w = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "precision_w = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_m = f1_score(y_test, y_pred_classes, average='macro')\n",
    "recall_m = recall_score(y_test, y_pred_classes, average='macro')\n",
    "precision_m = precision_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score (Weighted):\", f1_w)\n",
    "print(\"F1 Score (Macro):\", f1_m)\n",
    "print(\"Recall (Weighted):\", recall_w)\n",
    "print(\"Recall (Macro):\", recall_m)\n",
    "print(\"Precision (Weighted):\", precision_w)\n",
    "print(\"Precision (Macro):\", precision_m)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "emotion_categories = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_categories, yticklabels=emotion_categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e917c-5084-4f16-bbd7-67a4f825f0b2",
   "metadata": {},
   "source": [
    "# HistWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54713891-144a-48e5-95fc-7af86d49863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HistWords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Attention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('english_only.csv')\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATE_SIZE = 0.1765\n",
    "RANDOM_STATE_INT = 14988828\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "max_len = 128\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(df['excerpt_value_cleaned'])\n",
    "sequences = tokenizer.texts_to_sequences(df['excerpt_value_cleaned'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['plutchik_emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE_INT)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATE_SIZE, random_state=RANDOM_STATE_INT)\n",
    "\n",
    "def load_multiple_histwords_embeddings(embedding_files_path):\n",
    "    embedding_index = {}\n",
    "    for file in os.listdir(embedding_files_path):\n",
    "        if file.endswith('-vocab.pkl'):\n",
    "            decade = file.split('-vocab.pkl')[0]\n",
    "            vocab_path = os.path.join(embedding_files_path, file)\n",
    "            vectors_path = os.path.join(embedding_files_path, f\"{decade}-w.npy\")\n",
    "\n",
    "            if not os.path.exists(vectors_path):\n",
    "                print(f\"File {vectors_path} not found, skipping.\")\n",
    "                continue\n",
    "\n",
    "            with open(vocab_path, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "\n",
    "            word_vectors = np.load(vectors_path)\n",
    "\n",
    "            for i, word in enumerate(vocab):\n",
    "                if word not in embedding_index:\n",
    "                    embedding_index[word] = word_vectors[i]\n",
    "\n",
    "    return embedding_index\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_files_path = 'sgns/' \n",
    "embedding_index = load_multiple_histwords_embeddings(embedding_files_path)\n",
    "\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
    "bi_lstm = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
    "\n",
    "attention = Attention()([bi_lstm, bi_lstm])\n",
    "context_vector = GlobalAveragePooling1D()(attention)\n",
    "\n",
    "dropout_layer = Dropout(dropout_rate)(context_vector)\n",
    "dense_layer_1 = Dense(64, activation='relu')(dropout_layer)\n",
    "dropout_layer_2 = Dropout(dropout_rate)(dense_layer_1)\n",
    "output_layer = Dense(8, activation='softmax')(dropout_layer_2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_w = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_w = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "precision_w = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_m = f1_score(y_test, y_pred_classes, average='macro')\n",
    "recall_m = recall_score(y_test, y_pred_classes, average='macro')\n",
    "precision_m = precision_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score (Weighted):\", f1_w)\n",
    "print(\"F1 Score (Macro):\", f1_m)\n",
    "print(\"Recall (Weighted):\", recall_w)\n",
    "print(\"Recall (Macro):\", recall_m)\n",
    "print(\"Precision (Weighted):\", precision_w)\n",
    "print(\"Precision (Macro):\", precision_m)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "emotion_categories = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_categories, yticklabels=emotion_categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c304a2b-e4c9-4f24-8df4-bfc14b9017e0",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eda0cc-f186-4b99-a2af-e42003119998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Attention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('english_only.csv')\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATE_SIZE = 0.1765\n",
    "RANDOM_STATE_INT = 14988828\n",
    "batch_size = 32\n",
    "epochs = 10 \n",
    "\n",
    "max_len = 128\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(df['excerpt_value_cleaned'])\n",
    "sequences = tokenizer.texts_to_sequences(df['excerpt_value_cleaned'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['plutchik_emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE_INT)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATE_SIZE, random_state=RANDOM_STATE_INT)\n",
    "\n",
    "def load_fasttext_embeddings(fasttext_file_path, embedding_dim, tokenizer_word_index):\n",
    "    embeddings_index = {}\n",
    "    with open(fasttext_file_path, 'r', encoding='utf8') as f:\n",
    "        next(f)  # Skip the header row\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer_word_index) + 1, embedding_dim))\n",
    "    for word, i in tokenizer_word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 300 \n",
    "fasttext_file_path = 'crawl-300d-2M.vec'\n",
    "embedding_matrix = load_fasttext_embeddings(fasttext_file_path, embedding_dim, tokenizer.word_index)\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
    "bi_lstm = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
    "\n",
    "attention = Attention()([bi_lstm, bi_lstm])\n",
    "context_vector = GlobalAveragePooling1D()(attention)\n",
    "\n",
    "dropout_layer = Dropout(dropout_rate)(context_vector)\n",
    "dense_layer_1 = Dense(64, activation='relu')(dropout_layer)\n",
    "dropout_layer_2 = Dropout(dropout_rate)(dense_layer_1)\n",
    "output_layer = Dense(8, activation='softmax')(dropout_layer_2)  # 8 emotion classes from Plutchik's wheel of emotions\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_w = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_w = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "precision_w = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_m = f1_score(y_test, y_pred_classes, average='macro')\n",
    "recall_m = recall_score(y_test, y_pred_classes, average='macro')\n",
    "precision_m = precision_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score (Weighted):\", f1_w)\n",
    "print(\"F1 Score (Macro):\", f1_m)\n",
    "print(\"Recall (Weighted):\", recall_w)\n",
    "print(\"Recall (Macro):\", recall_m)\n",
    "print(\"Precision (Weighted):\", precision_w)\n",
    "print(\"Precision (Macro):\", precision_m)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "emotion_categories = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_categories, yticklabels=emotion_categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4d2c9-470b-40e6-a1b3-92e3292bcb59",
   "metadata": {},
   "source": [
    "# BERT or MacBERTh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259db05-8a5e-450e-bd03-e72c768b7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Dropout, Attention, GlobalAveragePooling1D, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(f'{len(physical_devices)} GPU(s) detected: {[device.name for device in physical_devices]}')\n",
    "else:\n",
    "    print('No GPU detected, using CPU.')\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/msc-thesis-dataset/english_only.csv') \n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATE_SIZE = 0.1765\n",
    "RANDOM_STATE_INT = 14988828\n",
    "batch_size = 32\n",
    "epochs = 10  \n",
    "\n",
    "max_len = 256\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_texts(texts, tokenizer, max_len):\n",
    "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_len)\n",
    "    return np.array(encodings['input_ids'])\n",
    "\n",
    "encoded_sequences = encode_texts(df['excerpt_value_cleaned'], tokenizer, max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['plutchik_emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE_INT)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATE_SIZE, random_state=RANDOM_STATE_INT)\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_layer = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "bert_outputs = Lambda(lambda x: bert_model(x)[0], output_shape=(max_len, 768))(input_layer)\n",
    "bi_lstm = Bidirectional(LSTM(128, return_sequences=True))(bert_outputs)\n",
    "\n",
    "attention = Attention()([bi_lstm, bi_lstm])\n",
    "context_vector = GlobalAveragePooling1D()(attention)\n",
    "\n",
    "dropout_layer = Dropout(dropout_rate)(context_vector)\n",
    "dense_layer_1 = Dense(64, activation='relu')(dropout_layer)\n",
    "dropout_layer_2 = Dropout(dropout_rate)(dense_layer_1)\n",
    "output_layer = Dense(8, activation='softmax')(dropout_layer_2)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_w = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_w = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "precision_w = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_m = f1_score(y_test, y_pred_classes, average='macro')\n",
    "recall_m = recall_score(y_test, y_pred_classes, average='macro')\n",
    "precision_m = precision_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score (Weighted):\", f1_w)\n",
    "print(\"F1 Score (Macro):\", f1_m)\n",
    "print(\"Recall (Weighted):\", recall_w)\n",
    "print(\"Recall (Macro):\", recall_m)\n",
    "print(\"Precision (Weighted):\", precision_w)\n",
    "print(\"Precision (Macro):\", precision_m)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "emotion_categories = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_categories, yticklabels=emotion_categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
