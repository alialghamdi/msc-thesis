{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd7593a-ae96-437a-8eac-a23c6f195475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alialghamdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alialghamdi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alialghamdi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m865.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Collecting fr-core-news-lg==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.7.0/fr_core_news_lg-3.7.0-py3-none-any.whl (571.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.8/571.8 MB\u001b[0m \u001b[31m866.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from fr-core-news-lg==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/alialghamdi/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "this is before cleaning\n",
      "this is before dates\n",
      "this is before cleaning links\n",
      "this is before cleaning locations\n",
      "this is before merging rows\n",
      "this is before mapping emotions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>graph</th>\n",
       "      <th>label</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_date</th>\n",
       "      <th>source_language</th>\n",
       "      <th>source_genre</th>\n",
       "      <th>relevantExcerpt</th>\n",
       "      <th>adjective</th>\n",
       "      <th>emotion</th>\n",
       "      <th>license</th>\n",
       "      <th>smellSource</th>\n",
       "      <th>carrier</th>\n",
       "      <th>excerpt_id</th>\n",
       "      <th>words</th>\n",
       "      <th>excerpt_value_cleaned</th>\n",
       "      <th>source_label_cleaned</th>\n",
       "      <th>country_codes</th>\n",
       "      <th>country_names</th>\n",
       "      <th>plutchik_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f17f9900-23af-5d6d-8b43-c9fa35a34fb2</td>\n",
       "      <td>british-library</td>\n",
       "      <td>[scent, sniff, odours, scented, smell]</td>\n",
       "      <td>f929e896-9202-5a7f-85c9-115b7d817892</td>\n",
       "      <td>1895.0</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[http://data.odeuropa.eu/source/f929e896-9202-...</td>\n",
       "      <td>[scented, sweet scented, dry]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'id': 'http://data.odeuropa.eu/vocabulary/ol...</td>\n",
       "      <td>[{'id': 'http://data.odeuropa.eu/vocabulary/ol...</td>\n",
       "      <td>534f2f41-50c4-5eaa-8e6c-40423ee4a90b</td>\n",
       "      <td>[odour, peculiar]</td>\n",
       "      <td>rera display mu anybody located 1 j yet hang c...</td>\n",
       "      <td>english seaman sixteenth century lecture deliv...</td>\n",
       "      <td>[IR]</td>\n",
       "      <td>[Iran]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>be527c46-15a4-58a2-950e-28f1854082b7</td>\n",
       "      <td>medical-heritage</td>\n",
       "      <td>[odour, ardour, perfumes]</td>\n",
       "      <td>448af7cf-722c-5997-9f09-8fcff3b8ce79</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[http://data.odeuropa.eu/source/448af7cf-722c-...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'id': 'http://data.odeuropa.eu/vocabulary/ol...</td>\n",
       "      <td>[]</td>\n",
       "      <td>56874547-bae7-54e3-aed4-5b3ec3c07365</td>\n",
       "      <td>[odour, of the breath, intensely foul]</td>\n",
       "      <td>abscess lung may declare jsudden discharge mat...</td>\n",
       "      <td>report 1924</td>\n",
       "      <td>[EG]</td>\n",
       "      <td>[Egypt]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f17f9900-23af-5d6d-8b43-c9fa35a34fb2</td>\n",
       "      <td>british-library</td>\n",
       "      <td>[scent, sniff, odours, scented, smell]</td>\n",
       "      <td>f929e896-9202-5a7f-85c9-115b7d817892</td>\n",
       "      <td>1895.0</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[http://data.odeuropa.eu/source/f929e896-9202-...</td>\n",
       "      <td>[scented, sweet scented, dry]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'id': 'http://data.odeuropa.eu/vocabulary/ol...</td>\n",
       "      <td>[{'id': 'http://data.odeuropa.eu/vocabulary/ol...</td>\n",
       "      <td>e4e63368-5bb1-5203-a537-acaa103f45c5</td>\n",
       "      <td>[smell, I, this]</td>\n",
       "      <td>beg said hope hurt plaguy thick see right arm ...</td>\n",
       "      <td>english seaman sixteenth century lecture deliv...</td>\n",
       "      <td>[IR]</td>\n",
       "      <td>[Iran]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7a1cee17-3f99-52bf-b68d-361d5b684785</td>\n",
       "      <td>medical-heritage</td>\n",
       "      <td>[stench]</td>\n",
       "      <td>e5ddddb9-91ce-54b1-99ac-e69c9dadfc0b</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>c9bba5ab-98e9-5cf6-9f3d-884da39806a6</td>\n",
       "      <td>intolerably offensive</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'id': 'http://data.odeuropa.eu/vocabulary/ol...</td>\n",
       "      <td>[]</td>\n",
       "      <td>01c6e4b9-1ee7-5422-a532-e9e066a6fdb3</td>\n",
       "      <td>[fetid, sweats, profuse fetid]</td>\n",
       "      <td>paet â  ¢ anything else occur twelfth day acc...</td>\n",
       "      <td>gynecological obstetrical surgical aspect pell...</td>\n",
       "      <td>[EG]</td>\n",
       "      <td>[Egypt]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f282ff75-d00f-5ba7-a225-7716777a0a4f</td>\n",
       "      <td>medical-heritage</td>\n",
       "      <td>[Perfume]</td>\n",
       "      <td>a5eeae69-0979-5be7-b651-5465af45767f</td>\n",
       "      <td>1865.0</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>b5709523-1985-54c4-b926-6757d5ba4860</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'id': 'http://data.odeuropa.eu/object/fe5c34...</td>\n",
       "      <td>bf243a6f-5fd3-53b8-ab8b-4e8da3b7f5bc</td>\n",
       "      <td>[fragrant, ointments]</td>\n",
       "      <td>perfumed water anointing washing body also use...</td>\n",
       "      <td>stammering stuttering nature treatment</td>\n",
       "      <td>[EG]</td>\n",
       "      <td>[Egypt]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id             graph  \\\n",
       "0  f17f9900-23af-5d6d-8b43-c9fa35a34fb2   british-library   \n",
       "1  be527c46-15a4-58a2-950e-28f1854082b7  medical-heritage   \n",
       "2  f17f9900-23af-5d6d-8b43-c9fa35a34fb2   british-library   \n",
       "3  7a1cee17-3f99-52bf-b68d-361d5b684785  medical-heritage   \n",
       "4  f282ff75-d00f-5ba7-a225-7716777a0a4f  medical-heritage   \n",
       "\n",
       "                                    label  \\\n",
       "0  [scent, sniff, odours, scented, smell]   \n",
       "1               [odour, ardour, perfumes]   \n",
       "2  [scent, sniff, odours, scented, smell]   \n",
       "3                                [stench]   \n",
       "4                               [Perfume]   \n",
       "\n",
       "                              source_id  source_date source_language  \\\n",
       "0  f929e896-9202-5a7f-85c9-115b7d817892       1895.0              en   \n",
       "1  448af7cf-722c-5997-9f09-8fcff3b8ce79       1924.0              en   \n",
       "2  f929e896-9202-5a7f-85c9-115b7d817892       1895.0              en   \n",
       "3  e5ddddb9-91ce-54b1-99ac-e69c9dadfc0b       1911.0              en   \n",
       "4  a5eeae69-0979-5be7-b651-5465af45767f       1865.0              en   \n",
       "\n",
       "  source_genre                                    relevantExcerpt  \\\n",
       "0           []  [http://data.odeuropa.eu/source/f929e896-9202-...   \n",
       "1           []  [http://data.odeuropa.eu/source/448af7cf-722c-...   \n",
       "2           []  [http://data.odeuropa.eu/source/f929e896-9202-...   \n",
       "3           []               c9bba5ab-98e9-5cf6-9f3d-884da39806a6   \n",
       "4           []               b5709523-1985-54c4-b926-6757d5ba4860   \n",
       "\n",
       "                       adjective emotion license  \\\n",
       "0  [scented, sweet scented, dry]      []    None   \n",
       "1                           None      []    None   \n",
       "2  [scented, sweet scented, dry]      []    None   \n",
       "3          intolerably offensive      []    None   \n",
       "4                           None      []    None   \n",
       "\n",
       "                                         smellSource  \\\n",
       "0  [{'id': 'http://data.odeuropa.eu/vocabulary/ol...   \n",
       "1  [{'id': 'http://data.odeuropa.eu/vocabulary/ol...   \n",
       "2  [{'id': 'http://data.odeuropa.eu/vocabulary/ol...   \n",
       "3  [{'id': 'http://data.odeuropa.eu/vocabulary/ol...   \n",
       "4                                                 []   \n",
       "\n",
       "                                             carrier  \\\n",
       "0  [{'id': 'http://data.odeuropa.eu/vocabulary/ol...   \n",
       "1                                                 []   \n",
       "2  [{'id': 'http://data.odeuropa.eu/vocabulary/ol...   \n",
       "3                                                 []   \n",
       "4  [{'id': 'http://data.odeuropa.eu/object/fe5c34...   \n",
       "\n",
       "                             excerpt_id  \\\n",
       "0  534f2f41-50c4-5eaa-8e6c-40423ee4a90b   \n",
       "1  56874547-bae7-54e3-aed4-5b3ec3c07365   \n",
       "2  e4e63368-5bb1-5203-a537-acaa103f45c5   \n",
       "3  01c6e4b9-1ee7-5422-a532-e9e066a6fdb3   \n",
       "4  bf243a6f-5fd3-53b8-ab8b-4e8da3b7f5bc   \n",
       "\n",
       "                                    words  \\\n",
       "0                       [odour, peculiar]   \n",
       "1  [odour, of the breath, intensely foul]   \n",
       "2                        [smell, I, this]   \n",
       "3          [fetid, sweats, profuse fetid]   \n",
       "4                   [fragrant, ointments]   \n",
       "\n",
       "                               excerpt_value_cleaned  \\\n",
       "0  rera display mu anybody located 1 j yet hang c...   \n",
       "1  abscess lung may declare jsudden discharge mat...   \n",
       "2  beg said hope hurt plaguy thick see right arm ...   \n",
       "3  paet â  ¢ anything else occur twelfth day acc...   \n",
       "4  perfumed water anointing washing body also use...   \n",
       "\n",
       "                                source_label_cleaned country_codes  \\\n",
       "0  english seaman sixteenth century lecture deliv...          [IR]   \n",
       "1                                        report 1924          [EG]   \n",
       "2  english seaman sixteenth century lecture deliv...          [IR]   \n",
       "3  gynecological obstetrical surgical aspect pell...          [EG]   \n",
       "4             stammering stuttering nature treatment          [EG]   \n",
       "\n",
       "  country_names plutchik_emotion  \n",
       "0        [Iran]             None  \n",
       "1       [Egypt]             None  \n",
       "2        [Iran]             None  \n",
       "3       [Egypt]             None  \n",
       "4       [Egypt]             None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import json\n",
    "from ast import literal_eval \n",
    "import requests\n",
    "import geopandas\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "spacy.cli.download(\"fr_core_news_lg\")\n",
    "\n",
    "# This is where you load the exported data\n",
    "file_path = 'all_languages/items.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# # Just\n",
    "# def collect_labels(obj, prefix='', label_dict=None):\n",
    "#     if label_dict is None:\n",
    "#         label_dict = {}\n",
    "\n",
    "#     if isinstance(obj, dict):\n",
    "#         for key, value in obj.items():\n",
    "#             label = prefix + '.' + key if prefix else key\n",
    "#             label_dict[label] = type(value).__name__\n",
    "#             collect_labels(value, label, label_dict)\n",
    "#     elif isinstance(obj, list):\n",
    "#         for item in obj:\n",
    "#             collect_labels(item, prefix, label_dict)\n",
    "\n",
    "#     return label_dict\n",
    "\n",
    "# label_hierarchy = collect_labels(data)\n",
    "\n",
    "# Reading from the exported JSON\n",
    "def normalize_data(entry):\n",
    "    base_info = {\n",
    "        'type': entry.get('@type'),\n",
    "        'id': entry.get('@id'),\n",
    "        'graph': entry.get('@graph'),\n",
    "        'label': entry.get('label'),\n",
    "        'source_id': entry.get('source', {}).get('@id'),\n",
    "        'source_label': entry.get('source', {}).get('label'),\n",
    "        'source_url': entry.get('source', {}).get('url'),\n",
    "        'source_date': entry.get('source', {}).get('date'),\n",
    "        'source_language': entry.get('source', {}).get('language'),\n",
    "        'source_genre': [genre.get('label') for genre in entry.get('source', {}).get('genre', [])],\n",
    "        'relevantExcerpt': entry.get('relevantExcerpt'),\n",
    "        'adjective': entry.get('adjective'),\n",
    "        'emotion': [emotion.get('label') for emotion in entry.get('emotion', [])],\n",
    "        'license': entry.get('license'),\n",
    "        'time': [{'id': time.get('@id'), 'label': time.get('label'), 'begin': time.get('begin'), 'end': time.get('end')} for time in entry.get('time', [])],\n",
    "        'place': [{'id': place.get('@id'), 'label': place.get('label')} for place in entry.get('place', [])],\n",
    "        'smellSource': [{'id': source.get('@id'), 'label': source.get('label')} for source in entry.get('smellSource', [])],\n",
    "        'carrier': [{'id': carrier.get('@id'), 'label': carrier.get('label'), 'exemplifies': carrier.get('exemplifies')} for carrier in entry.get('carrier', [])],\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for excerpt in entry.get('source', {}).get('excerpts', []):\n",
    "        excerpt_data = {\n",
    "            'excerpt_id': excerpt.get('@id'),\n",
    "            'excerpt_value': excerpt.get('value'),\n",
    "            'words': excerpt.get('words', [])\n",
    "        }\n",
    "\n",
    "        if not isinstance(excerpt_data['words'], list):\n",
    "            excerpt_data['words'] = [excerpt_data['words']]\n",
    "\n",
    "        row = {**base_info, **excerpt_data}\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "for entry in data:\n",
    "    rows.extend(normalize_data(entry))\n",
    "\n",
    "items_pd = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Filtering by languages (en, fr)\n",
    "filtered_df = items_pd[items_pd['source_language'] == 'en']\n",
    "#filtered_df = items_pd[items_pd['source_language'].isin(['en', 'fr'])]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# Load language models\n",
    "en_nlp = spacy.load(\"en_core_web_lg\")\n",
    "#fr_nlp = spacy.load(\"fr_core_news_lg\")\n",
    "\n",
    "# Initialize lemmatizers and stopwords\n",
    "en_lemmatizer = WordNetLemmatizer()\n",
    "#fr_lemmatizer = lambda text: \" \".join([token.lemma_ for token in fr_nlp(text)])\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "#fr_stopwords = set(stopwords.words('french'))\n",
    "\n",
    "print('this is before cleaning')\n",
    "##############\n",
    "# Cleaning function, for both languages\n",
    "# This is for the texts in excerpt_value and source_label\n",
    "##############\n",
    "def cleaning(text, language):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # No punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    if language == 'en':\n",
    "        # English preprocessing\n",
    "        # No stopwords\n",
    "        text = ' '.join([word for word in text.split() if word not in en_stopwords])\n",
    "        \n",
    "        # Lemmatize text\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = ' '.join([en_lemmatizer.lemmatize(w) for w in word_tokens])\n",
    "    \n",
    "    # elif language == 'fr':\n",
    "    #     # French preprocessing\n",
    "    #     # No stopwords\n",
    "    #     text = ' '.join([word for word in text.split() if word not in fr_stopwords])\n",
    "        \n",
    "    #     # Lemmatize text\n",
    "    #     text = fr_lemmatizer(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "filtered_df['excerpt_value_cleaned'] = filtered_df.apply(lambda row: cleaning(row['excerpt_value'], row['source_language']), axis=1)\n",
    "filtered_df['source_label_cleaned'] = filtered_df.apply(lambda row: cleaning(row['source_label'], row['source_language']), axis=1)\n",
    "\n",
    "print('this is before dates')\n",
    "############\n",
    "# This is for cleaning the year/date column, change varying formats to normal. If not possible, we get the median of the items in that \n",
    "# specific graph and then use as the date in the row.\n",
    "############\n",
    "\n",
    "# Preprocess 'source_date' to handle varying formats\n",
    "def preprocess_date(date):\n",
    "    if pd.isna(date):\n",
    "        return np.nan\n",
    "    if '/' in date:\n",
    "        return date.split('/')[0]  # Take the first year in case of '1855/1856'\n",
    "    if 'XX' in date:\n",
    "        return date.replace('XX', '50')  # Replace 'XX' with '50' for '18XX'\n",
    "    return date\n",
    "\n",
    "# Apply preprocessing to 'source_date'\n",
    "filtered_df['source_date'] = filtered_df['source_date'].apply(preprocess_date)\n",
    "\n",
    "# Convert 'source_date' to numeric, coerce errors to NaN\n",
    "filtered_df['source_date'] = pd.to_numeric(filtered_df['source_date'], errors='coerce')\n",
    "\n",
    "# Calculate the median year for each graph group\n",
    "median_years = filtered_df.groupby('graph')['source_date'].median()\n",
    "\n",
    "\n",
    "# A function to fill missing values with the median of the respective graph\n",
    "def fill_with_median(row):\n",
    "    if pd.isna(row['source_date']):\n",
    "        return median_years.get(row['graph'], np.nan)  # Get the median year, default to NaN if no median exists\n",
    "    else:\n",
    "        return row['source_date']\n",
    "\n",
    "# Apply the function to fill missing 'source_date' values\n",
    "filtered_df['source_date'] = filtered_df.apply(fill_with_median, axis=1)\n",
    "\n",
    "\n",
    "print('this is before cleaning links')\n",
    "############\n",
    "# Extracting the last part of the link\n",
    "############\n",
    "\n",
    "# Define a function to extract the last part of a URL\n",
    "def extract_last_part(url):\n",
    "    # Check if the entry is a string and contains a slash\n",
    "    if isinstance(url, str) and '/' in url:\n",
    "        return url.split('/')[-1]  # Split by '/' and return the last element\n",
    "    return url  # Return the original value if it's not a string or doesn't contain '/'\n",
    "\n",
    "# Columns to clean\n",
    "columns_to_clean = ['type', 'id', 'graph', 'source_id', 'relevantExcerpt', 'excerpt_id']\n",
    "\n",
    "# Apply the function to each specified column\n",
    "for column in columns_to_clean:\n",
    "    filtered_df[column] = filtered_df[column].apply(extract_last_part)\n",
    "\n",
    "print('this is before cleaning locations')\n",
    "############\n",
    "# Changing the place from exact areas to countries to simplify as it is not needed for this project\n",
    "# We will be using geonames dataset to transform them from the exact place to countries names and codes\n",
    "############\n",
    "\n",
    "# Function to extract the geoname ID from a URL\n",
    "def extract_geoname_id(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    return url.split('/')[-2]\n",
    "\n",
    "# Function to match the country code using a dictionary for fast lookup\n",
    "def match_country(geoname_id, geo_dict):\n",
    "    return geo_dict.get(geoname_id)\n",
    "\n",
    "# Function to process each place array and extract unique country codes\n",
    "def extract_and_match_countries(place_array, geo_dict):\n",
    "    country_codes = set()\n",
    "    for place in place_array:\n",
    "        geoname_id = extract_geoname_id(place['id'])\n",
    "        country_code = match_country(geoname_id, geo_dict)\n",
    "        if country_code:\n",
    "            country_codes.add(country_code)\n",
    "    return list(country_codes)\n",
    "\n",
    "# Process each row to extract country codes\n",
    "def process_row(place_array, geo_dict):\n",
    "    return extract_and_match_countries(place_array, geo_dict)\n",
    "\n",
    "# Load GeoNames data and convert to dictionary for faster access\n",
    "file_path = 'allCountries.txt'\n",
    "cols = ['geonameid', 'name', 'asciiname', 'alternatenames', 'latitude', 'longitude', \n",
    "        'feature class', 'feature code', 'country code', 'cc2', 'admin1', 'admin2', \n",
    "        'admin3', 'admin4', 'population', 'elevation', 'dem', 'timezone', 'modification date']\n",
    "geo_data = pd.read_csv(file_path, delimiter='\\t', names=cols, low_memory=False, encoding='utf-8', na_values='\\\\N', dtype={'geonameid': str})\n",
    "geo_dict = pd.Series(geo_data['country code'].values, index=geo_data['geonameid']).to_dict()\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize row processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    countries_results = list(executor.map(lambda x: process_row(x, geo_dict), filtered_df['place']))\n",
    "filtered_df['country_codes'] = countries_results\n",
    "\n",
    "middle_east_countries = {\n",
    "    'YE': 'Yemen',\n",
    "    'IQ': 'Iraq',\n",
    "    'SD': 'Sudan',\n",
    "    'SA': 'Saudi Arabia',\n",
    "    'PS': 'Palestine',  # Ensure Palestine is here\n",
    "    'JO': 'Jordan',\n",
    "    'OM': 'Oman',\n",
    "    'EG': 'Egypt',\n",
    "    'LB': 'Lebanon',\n",
    "    #'IL': 'Israel',  # This can be removed or handled differently\n",
    "    'AE': 'United Arab Emirates',\n",
    "    'SY': 'Syria',\n",
    "    'IR': 'Iran'\n",
    "}\n",
    "\n",
    "# Function to convert country codes to country names using the dictionary, handling \"IL\" specifically\n",
    "def codes_to_names(codes):\n",
    "    result = []\n",
    "    for code in codes:\n",
    "        if code == 'IL':  # Convert \"IL\" to \"PS\"\n",
    "            result.append('Palestine')\n",
    "        elif code in middle_east_countries:\n",
    "            result.append(middle_east_countries[code])\n",
    "    return result\n",
    "\n",
    "# Apply this function to each row in the 'country_codes' column\n",
    "filtered_df['country_names'] = filtered_df['country_codes'].apply(codes_to_names)\n",
    "\n",
    "print('this is before merging rows')\n",
    "##############\n",
    "# Merging and cleaning rows\n",
    "#\n",
    "##############\n",
    "\n",
    "def merge_complex_lists(lists):\n",
    "    result = []\n",
    "    seen = set()  # to track seen values for simple items or dict items\n",
    "    for sublist in lists:\n",
    "        if sublist is None:\n",
    "            continue  # Skip None values\n",
    "        for item in sublist:\n",
    "            if isinstance(item, dict):\n",
    "                # Convert dict to a tuple of its items to make it hashable\n",
    "                item_tuple = tuple(sorted(item.items()))\n",
    "                if item_tuple not in seen:\n",
    "                    seen.add(item_tuple)\n",
    "                    result.append(item)\n",
    "            else:\n",
    "                if item not in seen:\n",
    "                    seen.add(item)\n",
    "                    result.append(item)\n",
    "    return result\n",
    "\n",
    "def combine_rows(group):\n",
    "    first_row = group.iloc[0].copy()  # Base row for accumulation\n",
    "    for column in group.columns:\n",
    "        if column == 'label':\n",
    "            # Merge 'label' ensuring no duplicate strings and handling lists\n",
    "            combined_labels = set()\n",
    "            for entry in group[column]:\n",
    "                if isinstance(entry, list):\n",
    "                    combined_labels.update(entry)\n",
    "                elif isinstance(entry, str):\n",
    "                    combined_labels.add(entry)\n",
    "            first_row[column] = list(combined_labels)\n",
    "        elif column in ['smellSource', 'country_names', 'carrier', 'emotion']:\n",
    "            # Handle lists, possibly containing dicts\n",
    "            all_entries = group[column].tolist()\n",
    "            first_row[column] = merge_complex_lists(all_entries)\n",
    "        else:\n",
    "            # For other columns, take the first non-null value\n",
    "            non_null_values = group[column].dropna()\n",
    "            if not non_null_values.empty:\n",
    "                first_row[column] = non_null_values.iloc[0]\n",
    "    return first_row\n",
    "\n",
    "# Example DataFrame setup and applying the functions\n",
    "# Assuming filtered_df is previously defined with columns like 'excerpt_value', 'source_date', etc.\n",
    "# Apply the combination logic after normalizing data for processing\n",
    "filtered_df = filtered_df.groupby(['excerpt_value', 'source_date']).apply(combine_rows).reset_index(drop=True)\n",
    "\n",
    "print('this is before mapping emotions')\n",
    "############\n",
    "# Using the existing annotated emotions, we will clean it to map to the primary Plutchik emotion\n",
    "############\n",
    "\n",
    "primary_emotions_map = {\n",
    "    'calmness': 'joy', 'serenity': 'joy', 'joy': 'joy', 'ecstasy': 'joy',\n",
    "    'excitement': 'joy', 'relief': 'joy', 'despair': 'sadness', 'pensiveness': 'sadness',\n",
    "    'sadness': 'sadness', 'disappointment': 'sadness', 'grief': 'sadness', 'desire':'anticipation',\n",
    "    'embarrassment': 'fear', 'nostalgia': 'sadness', 'pain': 'sadness', 'greed':'anger',\n",
    "    'approval': 'trust', 'acceptance': 'trust', 'trust': 'trust', 'admiration': 'joy', 'courage':'anticipation',\n",
    "    'faith': 'trust', 'indifference': 'disgust', 'boredom': 'sadness', 'disgust': 'disgust',\n",
    "    'loathing': 'disgust', 'nervousness': 'fear', 'apprehension': 'fear', 'pride': 'joy',\n",
    "    'fear': 'fear', 'terror': 'fear', 'annoyance': 'anger', 'frustration': 'anger', 'love':'joy',\n",
    "    'anger': 'anger', 'rage': 'anger', 'envy': 'anger', 'surprise': 'surprise', 'gratitude':'joy',\n",
    "    'amazement': 'surprise', 'curiosity': 'anticipation', 'interest': 'anticipation', 'guilt':'sadness',\n",
    "    'anticipation': 'anticipation', 'vigilance': 'anticipation', 'doubt': 'fear', 'amusement':'joy',\n",
    "    'optimism': 'anticipation', 'disapproval': 'disgust'\n",
    "}\n",
    "\n",
    "# Apply the mapping with improved checks\n",
    "filtered_df['plutchik_emotion'] = filtered_df['emotion'].apply(\n",
    "    lambda x: primary_emotions_map[x[0]] if x and isinstance(x, list) and len(x) > 0 else None\n",
    ")\n",
    "\n",
    "filtered_df.drop(columns=['source_url', 'place', 'time', 'type', 'excerpt_value', 'source_label'], inplace=True)\n",
    "\n",
    "filtered_df.to_csv(f\"data_{pd.Timestamp.now().strftime('%Y-%m-%d_%H-%M')}.csv\")\n",
    "\n",
    "filtered_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
