{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7593a-ae96-437a-8eac-a23c6f195475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import json\n",
    "from ast import literal_eval \n",
    "import requests\n",
    "import geopandas\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# This is where you load the exported data\n",
    "file_path = 'all_languages/items.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# # Just\n",
    "# def collect_labels(obj, prefix='', label_dict=None):\n",
    "#     if label_dict is None:\n",
    "#         label_dict = {}\n",
    "\n",
    "#     if isinstance(obj, dict):\n",
    "#         for key, value in obj.items():\n",
    "#             label = prefix + '.' + key if prefix else key\n",
    "#             label_dict[label] = type(value).__name__\n",
    "#             collect_labels(value, label, label_dict)\n",
    "#     elif isinstance(obj, list):\n",
    "#         for item in obj:\n",
    "#             collect_labels(item, prefix, label_dict)\n",
    "\n",
    "#     return label_dict\n",
    "\n",
    "# label_hierarchy = collect_labels(data)\n",
    "\n",
    "# Reading from the exported JSON\n",
    "def normalize_data(entry):\n",
    "    base_info = {\n",
    "        'type': entry.get('@type'),\n",
    "        'id': entry.get('@id'),\n",
    "        'graph': entry.get('@graph'),\n",
    "        'label': entry.get('label'),\n",
    "        'source_id': entry.get('source', {}).get('@id'),\n",
    "        'source_label': entry.get('source', {}).get('label'),\n",
    "        'source_url': entry.get('source', {}).get('url'),\n",
    "        'source_date': entry.get('source', {}).get('date'),\n",
    "        'source_language': entry.get('source', {}).get('language'),\n",
    "        'source_genre': [genre.get('label') for genre in entry.get('source', {}).get('genre', [])],\n",
    "        'relevantExcerpt': entry.get('relevantExcerpt'),\n",
    "        'adjective': entry.get('adjective'),\n",
    "        'emotion': [emotion.get('label') for emotion in entry.get('emotion', [])],\n",
    "        'license': entry.get('license'),\n",
    "        'time': [{'id': time.get('@id'), 'label': time.get('label'), 'begin': time.get('begin'), 'end': time.get('end')} for time in entry.get('time', [])],\n",
    "        'place': [{'id': place.get('@id'), 'label': place.get('label')} for place in entry.get('place', [])],\n",
    "        'smellSource': [{'id': source.get('@id'), 'label': source.get('label')} for source in entry.get('smellSource', [])],\n",
    "        'carrier': [{'id': carrier.get('@id'), 'label': carrier.get('label'), 'exemplifies': carrier.get('exemplifies')} for carrier in entry.get('carrier', [])],\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for excerpt in entry.get('source', {}).get('excerpts', []):\n",
    "        excerpt_data = {\n",
    "            'excerpt_id': excerpt.get('@id'),\n",
    "            'excerpt_value': excerpt.get('value'),\n",
    "            'words': excerpt.get('words', [])\n",
    "        }\n",
    "\n",
    "        if not isinstance(excerpt_data['words'], list):\n",
    "            excerpt_data['words'] = [excerpt_data['words']]\n",
    "\n",
    "        row = {**base_info, **excerpt_data}\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "for entry in data:\n",
    "    rows.extend(normalize_data(entry))\n",
    "\n",
    "items_pd = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Filtering by languages (en, fr)\n",
    "filtered_df = items_pd[items_pd['source_language'].isin(['en', 'fr'])]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# Load language models\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "fr_nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Initialize lemmatizers and stopwords\n",
    "en_lemmatizer = WordNetLemmatizer()\n",
    "fr_lemmatizer = lambda text: \" \".join([token.lemma_ for token in fr_nlp(text)])\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "fr_stopwords = set(stopwords.words('french'))\n",
    "\n",
    "\n",
    "##############\n",
    "# Cleaning function, for both languages\n",
    "# This is for the texts in excerpt_value and source_label\n",
    "##############\n",
    "def cleaning(text, language):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # No punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    if language == 'en':\n",
    "        # English preprocessing\n",
    "        # No stopwords\n",
    "        text = ' '.join([word for word in text.split() if word not in en_stopwords])\n",
    "        \n",
    "        # Lemmatize text\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = ' '.join([en_lemmatizer.lemmatize(w) for w in word_tokens])\n",
    "    \n",
    "    elif language == 'fr':\n",
    "        # French preprocessing\n",
    "        # No stopwords\n",
    "        text = ' '.join([word for word in text.split() if word not in fr_stopwords])\n",
    "        \n",
    "        # Lemmatize text\n",
    "        text = fr_lemmatizer(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "filtered_df['excerpt_value_cleaned'] = filtered_df.apply(lambda row: cleaning(row['excerpt_value'], row['source_language']), axis=1)\n",
    "filtered_df['source_label_cleaned'] = filtered_df.apply(lambda row: cleaning(row['source_label'], row['source_language']), axis=1)\n",
    "\n",
    "\n",
    "############\n",
    "# This is for cleaning the year/date column, change varying formats to normal. If not possible, we get the median of the items in that \n",
    "# specific graph and then use as the date in the row.\n",
    "############\n",
    "\n",
    "# Preprocess 'source_date' to handle varying formats\n",
    "def preprocess_date(date):\n",
    "    if pd.isna(date):\n",
    "        return np.nan\n",
    "    if '/' in date:\n",
    "        return date.split('/')[0]  # Take the first year in case of '1855/1856'\n",
    "    if 'XX' in date:\n",
    "        return date.replace('XX', '50')  # Replace 'XX' with '50' for '18XX'\n",
    "    return date\n",
    "\n",
    "# Apply preprocessing to 'source_date'\n",
    "filtered_df['source_date'] = filtered_df['source_date'].apply(preprocess_date)\n",
    "\n",
    "# Convert 'source_date' to numeric, coerce errors to NaN\n",
    "filtered_df['source_date'] = pd.to_numeric(filtered_df['source_date'], errors='coerce')\n",
    "\n",
    "# Calculate the median year for each graph group\n",
    "median_years = filtered_df.groupby('graph')['source_date'].median()\n",
    "\n",
    "# A function to fill missing values with the median of the respective graph\n",
    "def fill_with_median(row):\n",
    "    if pd.isna(row['source_date']):\n",
    "        return median_years.get(row['graph'], np.nan)  # Get the median year, default to NaN if no median exists\n",
    "    else:\n",
    "        return row['source_date']\n",
    "\n",
    "# Apply the function to fill missing 'source_date' values\n",
    "filtered_df['source_date'] = filtered_df.apply(fill_with_median, axis=1)\n",
    "\n",
    "############\n",
    "# Extracting the last part of the link\n",
    "############\n",
    "\n",
    "# Define a function to extract the last part of a URL\n",
    "def extract_last_part(url):\n",
    "    # Check if the entry is a string and contains a slash\n",
    "    if isinstance(url, str) and '/' in url:\n",
    "        return url.split('/')[-1]  # Split by '/' and return the last element\n",
    "    return url  # Return the original value if it's not a string or doesn't contain '/'\n",
    "\n",
    "# Columns to clean\n",
    "columns_to_clean = ['type', 'id', 'graph', 'source_id', 'relevantExcerpt', 'excerpt_id']\n",
    "\n",
    "# Apply the function to each specified column\n",
    "for column in columns_to_clean:\n",
    "    filtered_df[column] = filtered_df[column].apply(extract_last_part)\n",
    "\n",
    "############\n",
    "# Changing the place from exact areas to countries to simplify as it is not needed for this project\n",
    "# We will be using geonames dataset to transform them from the exact place to countries names and codes\n",
    "############\n",
    "\n",
    "# Function to extract the geoname ID from a URL\n",
    "def extract_geoname_id(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    return url.split('/')[-2]\n",
    "\n",
    "# Function to match the country code using a dictionary for fast lookup\n",
    "def match_country(geoname_id, geo_dict):\n",
    "    return geo_dict.get(geoname_id)\n",
    "\n",
    "# Function to process each place array and extract unique country codes\n",
    "def extract_and_match_countries(place_array, geo_dict):\n",
    "    country_codes = set()\n",
    "    for place in place_array:\n",
    "        geoname_id = extract_geoname_id(place['id'])\n",
    "        country_code = match_country(geoname_id, geo_dict)\n",
    "        if country_code:\n",
    "            country_codes.add(country_code)\n",
    "    return list(country_codes)\n",
    "\n",
    "# Process each row to extract country codes\n",
    "def process_row(place_array, geo_dict):\n",
    "    return extract_and_match_countries(place_array, geo_dict)\n",
    "\n",
    "# Load GeoNames data and convert to dictionary for faster access\n",
    "file_path = 'allCountries.txt'\n",
    "cols = ['geonameid', 'name', 'asciiname', 'alternatenames', 'latitude', 'longitude', \n",
    "        'feature class', 'feature code', 'country code', 'cc2', 'admin1', 'admin2', \n",
    "        'admin3', 'admin4', 'population', 'elevation', 'dem', 'timezone', 'modification date']\n",
    "geo_data = pd.read_csv(file_path, delimiter='\\t', names=cols, low_memory=False, encoding='utf-8', na_values='\\\\N', dtype={'geonameid': str})\n",
    "geo_dict = pd.Series(geo_data['country code'].values, index=geo_data['geonameid']).to_dict()\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize row processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    countries_results = list(executor.map(lambda x: process_row(x, geo_dict), filtered_df['place']))\n",
    "filtered_df['country_codes'] = countries_results\n",
    "\n",
    "middle_east_countries = {\n",
    "    'YE': 'Yemen',\n",
    "    'IQ': 'Iraq',\n",
    "    'SD': 'Sudan',\n",
    "    'SA': 'Saudi Arabia',\n",
    "    'PS': 'Palestine',  # Ensure Palestine is here\n",
    "    'JO': 'Jordan',\n",
    "    'OM': 'Oman',\n",
    "    'EG': 'Egypt',\n",
    "    'LB': 'Lebanon',\n",
    "    # 'IL': 'Israel',  # This can be removed or handled differently\n",
    "    'AE': 'United Arab Emirates',\n",
    "    'SY': 'Syria',\n",
    "    'IR': 'Iran'\n",
    "}\n",
    "\n",
    "# Function to convert country codes to country names using the dictionary, handling \"IL\" specifically\n",
    "def codes_to_names(codes):\n",
    "    result = []\n",
    "    for code in codes:\n",
    "        if code == 'IL':  # Convert \"IL\" to \"PS\"\n",
    "            result.append('Palestine')\n",
    "        elif code in middle_east_countries:\n",
    "            result.append(middle_east_countries[code])\n",
    "    return result\n",
    "\n",
    "# Apply this function to each row in the 'country_codes' column\n",
    "filtered_df['country_names'] = filtered_df['country_codes'].apply(codes_to_names)\n",
    "\n",
    "##############\n",
    "# Merging and cleaning rows\n",
    "#\n",
    "##############\n",
    "\n",
    "def merge_complex_lists(lists):\n",
    "    result = []\n",
    "    seen = set()  # to track seen values for simple items or dict items\n",
    "    for sublist in lists:\n",
    "        if sublist is None:\n",
    "            continue  # Skip None values\n",
    "        for item in sublist:\n",
    "            if isinstance(item, dict):\n",
    "                # Convert dict to a tuple of its items to make it hashable\n",
    "                item_tuple = tuple(sorted(item.items()))\n",
    "                if item_tuple not in seen:\n",
    "                    seen.add(item_tuple)\n",
    "                    result.append(item)\n",
    "            else:\n",
    "                if item not in seen:\n",
    "                    seen.add(item)\n",
    "                    result.append(item)\n",
    "    return result\n",
    "\n",
    "def combine_rows(group):\n",
    "    first_row = group.iloc[0].copy()  # Base row for accumulation\n",
    "    for column in group.columns:\n",
    "        if column == 'label':\n",
    "            # Merge 'label' ensuring no duplicate strings and handling lists\n",
    "            combined_labels = set()\n",
    "            for entry in group[column]:\n",
    "                if isinstance(entry, list):\n",
    "                    combined_labels.update(entry)\n",
    "                elif isinstance(entry, str):\n",
    "                    combined_labels.add(entry)\n",
    "            first_row[column] = list(combined_labels)\n",
    "        elif column in ['smellSource', 'country_names', 'carrier', 'emotion']:\n",
    "            # Handle lists, possibly containing dicts\n",
    "            all_entries = group[column].tolist()\n",
    "            first_row[column] = merge_complex_lists(all_entries)\n",
    "        else:\n",
    "            # For other columns, take the first non-null value\n",
    "            non_null_values = group[column].dropna()\n",
    "            if not non_null_values.empty:\n",
    "                first_row[column] = non_null_values.iloc[0]\n",
    "    return first_row\n",
    "\n",
    "# Example DataFrame setup and applying the functions\n",
    "# Assuming filtered_df is previously defined with columns like 'excerpt_value', 'source_date', etc.\n",
    "# Apply the combination logic after normalizing data for processing\n",
    "filtered_df = filtered_df.groupby(['excerpt_value', 'source_date']).apply(combine_rows).reset_index(drop=True)\n",
    "\n",
    "############\n",
    "# Using the existing annotated emotions, we will clean it to map to the primary Plutchik emotion\n",
    "############\n",
    "\n",
    "primary_emotions_map = {\n",
    "    'calmness': 'joy', 'serenity': 'joy', 'joy': 'joy', 'ecstasy': 'joy',\n",
    "    'excitement': 'joy', 'relief': 'joy', 'despair': 'sadness', 'pensiveness': 'sadness',\n",
    "    'sadness': 'sadness', 'disappointment': 'sadness', 'grief': 'sadness', 'desire':'anticipation',\n",
    "    'embarrassment': 'fear', 'nostalgia': 'sadness', 'pain': 'sadness', 'greed':'anger',\n",
    "    'approval': 'trust', 'acceptance': 'trust', 'trust': 'trust', 'admiration': 'joy', 'courage':'anticipation',\n",
    "    'faith': 'trust', 'indifference': 'disgust', 'boredom': 'sadness', 'disgust': 'disgust',\n",
    "    'loathing': 'disgust', 'nervousness': 'fear', 'apprehension': 'fear', 'pride': 'joy',\n",
    "    'fear': 'fear', 'terror': 'fear', 'annoyance': 'anger', 'frustration': 'anger', 'love':'joy',\n",
    "    'anger': 'anger', 'rage': 'anger', 'envy': 'anger', 'surprise': 'surprise', 'gratitude':'joy',\n",
    "    'amazement': 'surprise', 'curiosity': 'anticipation', 'interest': 'anticipation', 'guilt':'sadness',\n",
    "    'anticipation': 'anticipation', 'vigilance': 'anticipation', 'doubt': 'fear', 'amusement':'joy',\n",
    "    'optimism': 'anticipation', 'disapproval': 'disgust'\n",
    "}\n",
    "\n",
    "# Apply the mapping with improved checks\n",
    "filtered_df['plutchik_emotion'] = filtered_df['emotion'].apply(\n",
    "    lambda x: primary_emotions_map[x[0]] if x and isinstance(x, list) and len(x) > 0 else None\n",
    ")\n",
    "\n",
    "filtered_df.drop(columns=['source_url', 'place', 'time', 'type', 'excerpt_value', 'source_label'], inplace=True)\n",
    "\n",
    "filtered_df.to_csv(f\"data_{pd.Timestamp.now().strftime('%Y-%m-%d_%H-%M')}.csv\")\n",
    "\n",
    "filtered_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
