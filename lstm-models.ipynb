{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0d9fba-49cd-4a70-9331-d99a7e4d1efa",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ece93-ff45-4496-8520-baaeeb4d5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('english_only.csv') \n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATE_SIZE = 0.1765\n",
    "RANDOM_STATE_INT = 14988828\n",
    "\n",
    "max_len = 128\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(df['excerpt_value_cleaned'])\n",
    "sequences = tokenizer.texts_to_sequences(df['excerpt_value_cleaned'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['plutchik_emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE_INT)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATE_SIZE, random_state=RANDOM_STATE_INT)\n",
    "\n",
    "embedding_dim = 100\n",
    "dropout_rate = 0.5\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2  \n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_weighted = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_weighted = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "precision_weighted = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred_classes, average='macro')\n",
    "precision_macro = precision_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score (Weighted):\", f1_weighted)\n",
    "print(\"Recall (Weighted):\", recall_weighted)\n",
    "print(\"Precision (Weighted):\", precision_weighted)\n",
    "print(\"F1 Score (Macro):\", f1_macro)\n",
    "print(\"Recall (Macro):\", recall_macro)\n",
    "print(\"Precision (Macro):\", precision_macro)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "emotion_categories = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_categories, yticklabels=emotion_categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc7682-a629-42a1-a455-60858e13236e",
   "metadata": {},
   "source": [
    "# Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0a75e-acc3-42a3-be7b-46153c3e254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('english_only.csv') \n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATE_SIZE = 0.1765\n",
    "RANDOM_STATE_INT = 14988828\n",
    "\n",
    "max_len = 128\n",
    "tokenizer = Tokenizer(num_words=None)  # Using all unique words\n",
    "tokenizer.fit_on_texts(df['excerpt_value_cleaned'])\n",
    "sequences = tokenizer.texts_to_sequences(df['excerpt_value_cleaned'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['plutchik_emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE_INT)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATE_SIZE, random_state=RANDOM_STATE_INT)\n",
    "\n",
    "embedding_dim = 100\n",
    "dropout_rate = 0.5\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_weighted = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_weighted = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "precision_weighted = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred_classes, average='macro')\n",
    "precision_macro = precision_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score (Weighted):\", f1_weighted)\n",
    "print(\"Recall (Weighted):\", recall_weighted)\n",
    "print(\"Precision (Weighted):\", precision_weighted)\n",
    "print(\"F1 Score (Macro):\", f1_macro)\n",
    "print(\"Recall (Macro):\", recall_macro)\n",
    "print(\"Precision (Macro):\", precision_macro)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "emotion_categories = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_categories, yticklabels=emotion_categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a52a449-29eb-49de-aeb4-6eae3f3f6375",
   "metadata": {},
   "source": [
    "# Bi-LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703783fd-abe1-4160-ba6c-fc2b84fbac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Attention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('english_only.csv') \n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATE_SIZE = 0.1765\n",
    "RANDOM_STATE_INT = 14988828\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "max_len = 128\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(df['excerpt_value_cleaned'])\n",
    "sequences = tokenizer.texts_to_sequences(df['excerpt_value_cleaned'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['plutchik_emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE_INT)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATE_SIZE, random_state=RANDOM_STATE_INT)\n",
    "\n",
    "embedding_dim = 100\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len)(input_layer)\n",
    "bi_lstm = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
    "\n",
    "attention = Attention()([bi_lstm, bi_lstm])\n",
    "context_vector = GlobalAveragePooling1D()(attention)\n",
    "\n",
    "dropout_layer = Dropout(dropout_rate)(context_vector)\n",
    "dense_layer_1 = Dense(64, activation='relu')(dropout_layer)\n",
    "dropout_layer_2 = Dropout(dropout_rate)(dense_layer_1)\n",
    "output_layer = Dense(8, activation='softmax')(dropout_layer_2)  # 8 emotion classes from Plutchik's wheel of emotions\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_w = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_w = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "precision_w = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_m = f1_score(y_test, y_pred_classes, average='macro')\n",
    "recall_m = recall_score(y_test, y_pred_classes, average='macro')\n",
    "precision_m = precision_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score (Weighted):\", f1_w)\n",
    "print(\"F1 Score (Macro):\", f1_m)\n",
    "print(\"Recall (Weighted):\", recall_w)\n",
    "print(\"Recall (Macro):\", recall_m)\n",
    "print(\"Precision (Weighted):\", precision_w)\n",
    "print(\"Precision (Macro):\", precision_m)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "emotion_categories = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_categories, yticklabels=emotion_categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90846a8a-6f48-44ff-8671-fef4497dffba",
   "metadata": {},
   "source": [
    "# Bi-LSTM with Attention and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e6fd0-55ea-4b38-af02-23bd6c85f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Attention, GlobalAveragePooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('english_only.csv')\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATE_SIZE = 0.1765\n",
    "RANDOM_STATE_INT = 14988828\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "max_len = 128\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(df['excerpt_value_cleaned'])\n",
    "sequences = tokenizer.texts_to_sequences(df['excerpt_value_cleaned'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['plutchik_emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE_INT)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATE_SIZE, random_state=RANDOM_STATE_INT)\n",
    "\n",
    "embedding_dim = 100\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len)(input_layer)\n",
    "\n",
    "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
    "\n",
    "bi_lstm = Bidirectional(LSTM(128, return_sequences=True))(pooling_layer)\n",
    "\n",
    "attention = Attention()([bi_lstm, bi_lstm])\n",
    "context_vector = GlobalAveragePooling1D()(attention)\n",
    "\n",
    "dropout_layer = Dropout(dropout_rate)(context_vector)\n",
    "dense_layer_1 = Dense(64, activation='relu')(dropout_layer)\n",
    "dropout_layer_2 = Dropout(dropout_rate)(dense_layer_1)\n",
    "output_layer = Dense(8, activation='softmax')(dropout_layer_2)  # 8 emotion classes from Plutchik's wheel of emotions\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_w = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_w = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "precision_w = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_m = f1_score(y_test, y_pred_classes, average='macro')\n",
    "recall_m = recall_score(y_test, y_pred_classes, average='macro')\n",
    "precision_m = precision_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score (Weighted):\", f1_w)\n",
    "print(\"F1 Score (Macro):\", f1_m)\n",
    "print(\"Recall (Weighted):\", recall_w)\n",
    "print(\"Recall (Macro):\", recall_m)\n",
    "print(\"Precision (Weighted):\", precision_w)\n",
    "print(\"Precision (Macro):\", precision_m)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "emotion_categories = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_categories, yticklabels=emotion_categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
